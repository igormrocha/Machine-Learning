{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
    "Gradient descent is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. To find a local minimum of a function using gradient descent, we take steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point.\n",
    "\n",
    "If we take the partial derivatives of the Linear Regression cost function with respect to m and b, we get\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\delta J}{\\delta m} = \\frac{2}{n} \\sum -x_i(y_i -(mx_i + b))  \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{\\delta J}{\\delta b} = \\frac{2}{n} \\sum -(y_i -(mx_i + b))  \n",
    "\\end{equation}\n",
    "\n",
    "The iterative steps are simply:\n",
    "\n",
    "m = m - learning_rate * $\\frac{\\delta J}{\\delta m}$ \n",
    "\n",
    "b = b - learning_rate * $\\frac{\\delta J}{\\delta b}$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(x,y):\n",
    "    m_curr = b_curr = 0\n",
    "    iterations = 10000\n",
    "    n = len(x)\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    for i in range(iterations):\n",
    "        y_predicted = m_curr * x + b_curr\n",
    "        cost = (1/n) * sum([val**2 for val in (y-y_predicted)])\n",
    "        md = -(2/n)*sum(x*(y-y_predicted))\n",
    "        bd = -(2/n)*sum(y-y_predicted)\n",
    "        m_curr = m_curr - learning_rate * md\n",
    "        b_curr = b_curr - learning_rate * bd\n",
    "        print (\"m {}, b {}, cost {} iteration {}\".format(m_curr,b_curr,cost, i))\n",
    "\n",
    "x = np.array([1,2,3,4,5])\n",
    "y = np.array([5,7,9,11,13])\n",
    "\n",
    "gradient_descent(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression vs Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we take the output of the linear function and squash the value within the range of [0,1] using the sigmoid function. The chosen class will be the one of maximum likelihood. \n",
    "\n",
    "SVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data.\n",
    "\n",
    "SVM is deterministic while Logistic Regression is statistical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
