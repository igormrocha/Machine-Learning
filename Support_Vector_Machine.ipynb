{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "The goal of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N â€” the number of features) that distinctly classifies the data points.\n",
    "The nearby datapoints are called Support Vectors, hence the name support vector machine.\n",
    "Our objective is to find a plane that has the maximum margin, i.e the maximum distance between data points of both classes. \n",
    "\n",
    "![title](svm.png)\n",
    "\n",
    "The gamma parameter defines how far the influence of a single training example reaches. \n",
    "\n",
    "High Gamma will consider only points close to the plausible hyperplane.\n",
    "\n",
    "Low Gamma will consider points at greater distance.\n",
    "\n",
    "Low regularization (C): Small C makes the cost of misclassificaiton low (\"soft margin\"), thus allowing more of them for the sake of wider \"cushion\".\n",
    "\n",
    "High regularization (C): Large C makes the cost of misclassification high ('hard margin\"), thus forcing the algorithm to explain the input data stricter and potentially overfit.\n",
    "\n",
    "It is also possible to perform transformations on the existing features, for example: $z = x^2 + y^2$, intentionally adding another dimension. This transformation is called kernel. The kernel function is what is applied on each data instance to map the __original non-linear__ observations into a __higher-dimensional space__ in which they become __separable linearly__. More about kernels on https://stackoverflow.com/questions/33778297/support-vector-machine-kernel-types.\n",
    "\n",
    "SVMs are inherently two-class classifiers. In particular, the most common technique in practice has been to build $N$ one-versus-rest classifiers (commonly referred to as one-versus-all or OVA classification), and to choose the class which classifies the test datum with greatest margin. Another strategy is to build a set of one-versus-one classifiers, and to choose the class that is selected by the most classifiers. While this involves building $N(N-1)/2$ classifiers, the time for training classifiers may actually decrease, since the training data set for each classifier is much smaller.\n",
    "\n",
    "__Advantage__: when using SVM we don't need all the datapoints, only the Support Vectors, thus decreasing the complexity and time needed.\n",
    "\n",
    "__Example description__: Guess the number shown in the image based on the pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "digits = load_digits()\n",
    "dir(digits)\n",
    "plt.gray();\n",
    "plt.imshow(digits.images[0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size = 0.2)\n",
    "\n",
    "model = SVC(C=1.0, kernel='linear')\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "plt.figure(figsize=(10,7));\n",
    "sns.heatmap(cm, annot = True);\n",
    "# The Linear SVM algorithm presented a slightly better peformance than the Logistic Regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Math behind SVM__\n",
    "\n",
    "__Support Vector Machine is a Convex Optimization Problem__\n",
    "\n",
    "First, we define two hyperplanes which are the boundaries for each class group (y=1 or y=-1). The two hyperplanes are described by __W__ and b as following:\n",
    "\n",
    "\\begin{equation}\n",
    "W \\cdot X_+ + b >= 1 \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "W \\cdot X_{-} + b <= -1\n",
    "\\end{equation}\n",
    "\n",
    "Multiplying these two equations by y we get the constraint:\n",
    "\n",
    "\\begin{equation}\n",
    "y \\cdot (W \\cdot X + b) - 1 => 0\n",
    "\\end{equation}\n",
    "\n",
    "And the __separating hyperplane__ is given by:\n",
    "\n",
    "$$ W \\cdot X + b = 0 $$\n",
    "\n",
    "Where W is perpendicular to the hyperplane.\n",
    "\n",
    "__To find the width__ of the margin, we can do as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "(xb - xr) \\cdot \\frac{W}{|W|}\n",
    "\\end{equation}\n",
    "\n",
    "Where xb and xr are vectors lying on the boundary, therefore the above equation yields in:\n",
    "\n",
    "Width $ = \\frac{2}{|W|} $, and in order to maximize the width we should minimize |W|. However, for mathematical convenience we say that we want to minimize $$ \\frac{1}{2} ||W||^2 $$\n",
    "\n",
    "From there we use Lagrange multipliers to obtain the values of __W__  and b that maximize the margin and satisfy the constraints.\n",
    "\n",
    "$ L = \\frac{1}{2}||w||^2 - \\sum \\alpha_i [y_i(w \\cdot x + b)-1] $\n",
    "\n",
    "Deriving with respect to w and b and equaling it to zero, we get:\n",
    "\n",
    "$ w = \\sum \\alpha_i y_i x_i $   and   $ \\sum \\alpha_i y_i = 0 $\n",
    "\n",
    "Thus, \n",
    "\n",
    "$ L = \\sum{} \\alpha_i - \\frac{1}{2} \\sum \\sum \\alpha_i \\alpha_j y_i y_j x_i \\cdot x_j $\n",
    "\n",
    "More on: https://towardsdatascience.com/mathematics-behind-svm-support-vector-machines-84742ddda0ca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------- SVM from scracth -----------------#\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.externals.joblib import parallel_backend\n",
    "\n",
    "class Support_Vector_Machine:\n",
    "    def __init__(self, visualization = True):\n",
    "        self.visualization = visualization\n",
    "        self.colors = {1:'r',-1:'b'}\n",
    "        if self.visualization:\n",
    "            self.fig = plt.figure()\n",
    "            self.ax = self.fig.add_subplot(1,1,1)\n",
    "    \n",
    "    def fit(self, data):\n",
    "        self.data = data\n",
    "        opt_dict = {}\n",
    "        transforms = [[1,1],\n",
    "                      [-1,1],\n",
    "                      [-1,-1],\n",
    "                      [1,-1]]\n",
    "        \n",
    "        all_data = []            # Assuming a dict will be passed \n",
    "        for yi in self.data:\n",
    "            for feature_set in self.data[yi]:\n",
    "                for feature in feature_set:\n",
    "                    all_data.append(feature)\n",
    "        \n",
    "        self.max_feature_value = max(all_data)\n",
    "        self.min_feature_value = min(all_data)\n",
    "        all_data = None\n",
    "        \n",
    "        step_sizes = [self.max_feature_value * 0.1,\n",
    "                      self.max_feature_value * 0.01,\n",
    "                      self.max_feature_value * 0.001,                   \n",
    "                     ]\n",
    "        \n",
    "        b_range_multiple = 5\n",
    "        # we dont need to take as small of steps\n",
    "        # with b as we do with w\n",
    "        b_multiple = 5\n",
    "        latest_optimum = [self.max_feature_value*10, self.max_feature_value*10]\n",
    "        \n",
    "        for step in step_sizes:\n",
    "            w = np.array([latest_optimum[0], latest_optimum[1]]).astype(float)\n",
    "            for w[0] in np.arange(latest_optimum[0],0,-step):\n",
    "                for w[1] in np.arange(latest_optimum[1],0, -step):\n",
    "                    for b in np.arange(-1*(self.max_feature_value*b_range_multiple),\n",
    "                                       self.max_feature_value*b_range_multiple,\n",
    "                                       step*b_multiple):\n",
    "                        for tr in transforms:\n",
    "                            w_t = w*tr\n",
    "                            found_option = True\n",
    "                            # yi(xi.w+b) >= 1\n",
    "                            for i in self.data:\n",
    "                                for xi in self.data[i]:\n",
    "                                    yi = i\n",
    "                                    if not yi*(np.dot(w_t,xi)+b) >= 1:  # This condition should be respected for all\n",
    "                                        found_option = False            # data points\n",
    "                                        break\n",
    "                                else:\n",
    "                                    continue\n",
    "                                break\n",
    "\n",
    "                            if found_option:\n",
    "                                opt_dict[np.linalg.norm(w_t)] = [w_t,b]\n",
    "                 \n",
    "            norms = sorted([n for n in opt_dict])\n",
    "            opt_choice = opt_dict[norms[0]] # Get the values of opt_dict for the optimal w\n",
    "            # opt_w: [w,b]\n",
    "            self.w = opt_choice[0]\n",
    "            self.b = opt_choice[1]\n",
    "            latest_optimum[0] = abs(opt_choice[0][0]) + 2*step\n",
    "            latest_optimum[1] = abs(opt_choice[0][1]) + 2*step\n",
    "\n",
    "    def predict(self, features):\n",
    "        # sign (x.w + b)\n",
    "        classification = np.sign(np.dot(np.array(features), self.w) + self.b)\n",
    "        if classification !=0 and self.visualization:\n",
    "                self.ax.scatter(features[0], features[1], s=200, marker='*', c=self.colors[classification])\n",
    "        return classification\n",
    "    \n",
    "    def visualize(self):\n",
    "        [[self.ax.scatter(x[0],x[1], s=100, color = self.colors[i]) for x in data_dict[i]] for i in data_dict]\n",
    "        \n",
    "        # v = x.w + b\n",
    "        def hyperplane(x0,w,b,v):            # Returns the SV after calculating the 2nd\n",
    "            x1 = (-w[0]*x0 - b + v)/ w[1]    # coordinate (1st is given)\n",
    "            return x0,x1\n",
    "    \n",
    "        datarange = (self.min_feature_value*0.9, self.max_feature_value*1.1)\n",
    "        hyp_x_min = datarange[0]\n",
    "        hyp_x_max = datarange[1]\n",
    "        \n",
    "        # positive support vectors\n",
    "        psv1 = hyperplane(hyp_x_min, self.w, self.b, 1)\n",
    "        psv2 = hyperplane(hyp_x_max, self.w, self.b, 1)\n",
    "        self.ax.plot([psv1[0],psv2[0]],[psv1[1],psv2[1]])\n",
    "        \n",
    "        #negative support vectors\n",
    "        nsv1 = hyperplane(hyp_x_min, self.w, self.b, -1)\n",
    "        nsv2 = hyperplane(hyp_x_max, self.w, self.b, -1)\n",
    "        self.ax.plot([nsv1[0],nsv2[0]],[nsv1[1],nsv2[1]])\n",
    "        \n",
    "        # hyperplane\n",
    "        hp1 = hyperplane(hyp_x_min, self.w, self.b, 0)\n",
    "        hp2 = hyperplane(hyp_x_max, self.w, self.b, 0)\n",
    "        self.ax.plot([hp1[0],hp2[0]],[hp1[1],hp2[1]])\n",
    "\n",
    "\n",
    "data_dict = {-1:np.array([[2,7],\n",
    "                         [3,8],\n",
    "                         [2.5,6]]),\n",
    "              1:np.array([[7,1],\n",
    "                          [6,-1],\n",
    "                          [5,1]])\n",
    "            }\n",
    "\n",
    "svm = Support_Vector_Machine()\n",
    "svm.fit(data=data_dict)\n",
    "svm.visualize()\n",
    "\n",
    "predict_points = [[1,1],[5,1],[6,5],[9,1],[1,9]]\n",
    "\n",
    "#for p in predict_points:\n",
    "#    svm.predict(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
